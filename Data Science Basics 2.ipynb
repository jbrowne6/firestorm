{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38532bitf0de9328736a43389da4d334482fc10d",
   "display_name": "Python 3.8.5 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Machine Learning\n",
    "Machine learning can be broken into two categories: unsupervised and supervised. Unsupervised learning uses unlabeled data. Two goals of unsupervised learning are anomaly detection and clustering. For instance, imagine the iris dataset didn't have the species feature (i.e. didn't have labels). Anomaly detection could tell us which of the 150 instances stand out from the rest of the data. Clustering could tell us that there are 3 distinct groups of instances in the data, thereby recovering the unknown class labels.\n",
    "\n",
    "We are going to focus on supervised learning for now. The main goal of supervised learning is to classify instances. In other words, given labeled data, we create a model that will label unlabeled instances.\n",
    "\n",
    "Lets look at an example.\n",
    "<br><br>\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "loaded_data = datasets.load_iris()\n",
    "\n",
    "x = loaded_data.data # By convention we store instances in a variable called x\n",
    "y = loaded_data.target # By convention we store labels in a variable called y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([5.1, 3.5, 1.4, 0.2])"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "x[0] # This is the first instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "y[0] # This is the label for the first instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "setosa\n"
    }
   ],
   "source": [
    "print(loaded_data.target_names[y[0]]) # Remember, 0 means setosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## Train a model\n",
    "\n",
    "<br>\n",
    "\n",
    "### First we setup the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "# We are going to set up a nearest neighbors classifier\n",
    "nn_model = neighbors.KNeighborsClassifier(3) # The '3' specifies how many neighbors to use when determining an instances class. All model types have parameters that a user sets prior to training. These user settable parameters are called hyperparameters. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "KNeighborsClassifier(n_neighbors=3)"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "nn_model.fit(x, y) # The data we use to train a model is called the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a model, let's use it to classify some instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_model.predict(x) # Have the model create labels for the instances. The data we use to test a model is called the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n"
    }
   ],
   "source": [
    "print(y) # print out the known labels\n",
    "print(predictions) # print out the model generated labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets generate a single value to represent how well our model worked.\n",
    "This value is called accuracy. There are better measurements of how well models perform. We'll look at these later. For now, accuracy is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The accuracy is 0.96\n"
    }
   ],
   "source": [
    "# This code iterates over the predictions and labels, adding 1 to num_correct when the prediction matches the corresponding label\n",
    "num_predictions = len(predictions)\n",
    "num_correct = 0\n",
    "\n",
    "for instance_num in range(num_predictions):\n",
    "    if predictions[instance_num] == y[instance_num]:\n",
    "        num_correct += 1\n",
    "\n",
    "print(f\"The accuracy is {num_correct / num_predictions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The accuracy is 0.96\n"
    }
   ],
   "source": [
    "# Or we can use sklearn's function to do the same\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"The accuracy is {accuracy_score(y, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Okay, that wasn't that impressive. Why? We created a model using 150 instances, then checked our model using those same 150 instances. What's wrong with this? Above we used the nearest neighbor (NN) classifier. If you knew every new unclassified instance exactly matches one of the training instances, could you design a method better than NN? Think about it for a minute. One better solution would be to just look up the label of the matching training instance and assign that label to the new instance.\n",
    "\n",
    "<br>\n",
    "\n",
    "A better test of our model would be to train the model with some labeled data, then test it with a different set of labeled data. Instead of collecting new instances, we will randomly split the data into two different sets: training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[5.7 3.8 1.7 0.3]\n"
    }
   ],
   "source": [
    "## Split Data into testing and training sets\n",
    "seed = 49\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.20, random_state=seed)\n",
    "\n",
    "\"\"\"\n",
    "This function is built into sklearn. It randomly choose 80% of the instances for the training set and uses the remaining 20% (test_size = .20) for the test set.\n",
    "\n",
    "The seed is an optional argument which guarantees that the random selections are always the same. \n",
    "\"\"\"\n",
    "print(x_train[0]) # run this cell several times and this will print the same instance every time.\n",
    "\n",
    "# Now change the seed to any other number and try it again. x_train[0] is now a different instance. Using a seed allows us to perform reproducible science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train again\n",
    "Now we are going to train a new model using the training set then test using the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "KNeighborsClassifier(n_neighbors=3)"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "\n",
    "nn_model = neighbors.KNeighborsClassifier(3)\n",
    "\n",
    "nn_model.fit(x_train, y_train) # train the model using the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1 2 1 2 2 0 2 2 2 2 0 1 0 1 1 1 1 2 2 0 0 1 0 2 0 1 1 2 0 2]\n[1 2 1 2 2 0 2 2 2 2 0 1 0 1 1 2 1 2 2 0 0 1 0 2 0 1 1 1 0 2]\n"
    }
   ],
   "source": [
    "predictions = nn_model.predict(x_test)\n",
    "print(predictions)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The accuracy is 0.9333333333333333\n"
    }
   ],
   "source": [
    "# Or we can use sklearn's function to do the same\n",
    "print(f\"The accuracy is {accuracy_score(y_test, predictions)}\")"
   ]
  }
 ]
}